{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build a voice assistant that responds to basic inquiries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Collecting comtypes; platform_system == \"Windows\"\n",
      "  Downloading comtypes-1.1.7.zip (180 kB)\n",
      "Requirement already satisfied: pywin32; platform_system == \"Windows\" in c:\\users\\sakshi\\anaconda3\\envs\\sakshienv\\lib\\site-packages (from pyttsx3) (227)\n",
      "Collecting pypiwin32; platform_system == \"Windows\"\n",
      "  Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Building wheels for collected packages: comtypes\n",
      "  Building wheel for comtypes (setup.py): started\n",
      "  Building wheel for comtypes (setup.py): finished with status 'done'\n",
      "  Created wheel for comtypes: filename=comtypes-1.1.7-py3-none-any.whl size=164608 sha256=a27d1c2dff71d4ce55cfcd927a084cc08ad3c2a1598e15dfde2181c7505e2035\n",
      "  Stored in directory: c:\\users\\sakshi\\appdata\\local\\pip\\cache\\wheels\\01\\6c\\bd\\be8ac2d1cf71f2231e4d7d692c12235699259c8358ce624092\n",
      "Successfully built comtypes\n",
      "Installing collected packages: comtypes, pypiwin32, pyttsx3\n",
      "Successfully installed comtypes-1.1.7 pypiwin32-223 pyttsx3-2.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import speech_recognition as sr \n",
    "from datetime import date\n",
    "import webbrowser\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weather Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather(city):\n",
    "    url = 'https://www.google.com/search?q=accuweather+' + city\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    links = [a['href']for a in soup.findAll('a')]\n",
    "    link = str(links[16])\n",
    "    link = link.split('=')\n",
    "    link = str(link[1]).split('&')\n",
    "    link = link[0]\n",
    "    \n",
    "    page = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'lxml') \n",
    "    \n",
    "    time = soup.find('p', attrs = {'class': 'cur-con-weather-card__subtitle'})\n",
    "    time = re.sub('\\n', '', time.text)\n",
    "    time = re.sub('\\t', '', time)\n",
    "    time = 'Time: ' + time\n",
    "\n",
    "    temperature = soup.find('div', attrs = {'class':'temp'})\n",
    "    temperature = 'Temperature: ' + temperature.text\n",
    "    \n",
    "    realfeel = soup.find('div', attrs = {'class': 'real-feel'})\n",
    "    realfeel = re.sub('\\n', '',realfeel.text)\n",
    "    realfeel = re.sub('\\t', '',realfeel)\n",
    "    realfeel = 'RealFeel: ' + realfeel[-3:] + 'C'\n",
    "\n",
    "    climate = soup.find('span', attrs = {'class':'phrase'})\n",
    "    climate = \"Climate: \" + climate.text\n",
    "    \n",
    "    info = 'For more information visit: ' + link \n",
    "    \n",
    "    print('The weather for today is: ')\n",
    "    print(time)\n",
    "    print(temperature)\n",
    "    print(realfeel)\n",
    "    print(climate)\n",
    "    print(info)\n",
    "    engine.say('The weather for today is: ')\n",
    "    engine.say(time)\n",
    "    engine.say(temperature)\n",
    "    engine.say(realfeel)\n",
    "    engine.say(climate)\n",
    "    engine.say('For more information visit accuweather.com' )\n",
    "    engine.runAndWait()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Corona Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corona_updates(audio):\n",
    "    \n",
    "    audio = audio\n",
    "\n",
    "    url = 'https://www.worldometers.info/coronavirus/'\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    totalcases = soup.findAll('div', attrs =  {'class': 'maincounter-number'})\n",
    "    total_cases = []\n",
    "    for total in totalcases:\n",
    "        total_cases.append(total.find('span').text)\n",
    "\n",
    "    world_total = 'Total Coronavirus Cases: ' + total_cases[0]\n",
    "    world_deaths = 'Total Deaths: ' + total_cases[1]\n",
    "    world_recovered = 'Total Recovered: ' + total_cases[2]\n",
    "    \n",
    "    info = 'For more information visit: ' + 'https://www.worldometers.info/coronavirus/#countries'\n",
    "\n",
    "    if 'world' in audio:\n",
    "        print('World Updates: ')\n",
    "        print(world_total)\n",
    "        print(world_deaths)\n",
    "        print(world_recovered)\n",
    "        print(info)\n",
    "\n",
    "    else:\n",
    "        country = audio\n",
    "\n",
    "        url = 'https://www.worldometers.info/coronavirus/country/' + country.lower() + '/'\n",
    "        page = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "        totalcases = soup.findAll('div', attrs =  {'class': 'maincounter-number'})\n",
    "        total_cases = []\n",
    "        for total in totalcases:\n",
    "            total_cases.append(total.find('span').text)    \n",
    "\n",
    "        total = 'Total Coronavirus Cases: ' + total_cases[0]\n",
    "        deaths = 'Total Deaths: ' + total_cases[1]\n",
    "        recovered = 'Total Recovered: ' + total_cases[2]\n",
    "\n",
    "        info = 'For more information visit: ' + url\n",
    "\n",
    "        updates = country + ' Updates: '\n",
    "\n",
    "        print(updates)\n",
    "        print(total)\n",
    "        print(deaths)\n",
    "        print(recovered)\n",
    "        print(info)\n",
    "        \n",
    "        engine.say(updates)\n",
    "        engine.say(total)\n",
    "        engine.say(deaths)\n",
    "        engine.say(recovered)\n",
    "        engine.say('For more information visit: worldometers.info')\n",
    "        engine.runAndWait()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Meaning of a Word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_meaning(audio):\n",
    "    word = audio\n",
    "    url = 'https://www.dictionary.com/browse/' + word\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    soup\n",
    "    meanings = soup.findAll('div', attrs =  {'class': 'css-1o58fj8 e1hk9ate4'})\n",
    "    meaning = [x.text for x in meanings]\n",
    "    first_meaning = meaning[0]\n",
    "    for x in meaning:\n",
    "        print(x)\n",
    "        print('\\n')\n",
    "    engine.say(first_meaning)\n",
    "    engine.runAndWait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_notes():\n",
    "\n",
    "    r5 = sr.Recognizer()  \n",
    "    with sr.Microphone() as source:\n",
    "        print('What is your \"TO DO LIST\" for today')\n",
    "        engine.say('What is your \"TO DO LIST\" for today')\n",
    "        engine.runAndWait()\n",
    "        audio = r5.listen(source)\n",
    "        audio = r5.recognize_google(audio)\n",
    "        print(audio)\n",
    "        today = date.today()\n",
    "        today = str(today)\n",
    "        with open('MyNotes.txt','a') as f:\n",
    "            f.write('\\n')\n",
    "            f.write(today)\n",
    "            f.write('\\n')\n",
    "            f.write(audio)\n",
    "            f.write('\\n')\n",
    "            f.write('......')\n",
    "            f.write('\\n')\n",
    "            f.close() \n",
    "        engine.say('Notes Taken')\n",
    "        engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_notes():\n",
    "    with open('MyNotes.txt', 'r') as f:\n",
    "        task = f.read()\n",
    "        task = task.split('......')\n",
    "    engine.say(task[-2])\n",
    "    engine.runAndWait()\n",
    "    webbrowser.open('http://localhost:8888/edit/Untitled%20Folder%201/MyNotes.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news():\n",
    "    url = 'https://news.google.com/topstories?hl=en-IN&gl=IN&ceid=IN:en '\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    news = soup.findAll('h3', attrs = {'class':'ipQwMb ekueJc RD0gLb'})\n",
    "    for n in news:\n",
    "        print(n.text)\n",
    "        print('\\n')\n",
    "        engine.say(n.text)\n",
    "    print('For more information visit: ', url)\n",
    "    engine.say('For more information visit google news')\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Youtube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_youtube(audio):\n",
    "\n",
    "    url = 'https://www.google.com/search?q=youtube+' + audio\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
    "    }\n",
    "    engine.say('Playing')\n",
    "    engine.say(audio)\n",
    "    engine.runAndWait()\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    link = soup.findAll('div', attrs = {'class':'r'})\n",
    "    link = link[0]\n",
    "    link = link.find('a')\n",
    "    link = str(link)\n",
    "    link = link.split('\"')\n",
    "    link = link[1]\n",
    "\n",
    "    webbrowser.open(link) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening..\n",
      "..\n",
      "scraping\n",
      "the act of a person or thing that scrapes. the sound of something being scraped. Usually scrapings. something that is scraped off, up, or together.Digital Technology. the process of extracting data from a digital source for automated replication, formatting, or manipulation by a computer program, as in data mining or website data analysis: screen scraping;web scraping;URL scraping.\n",
      "\n",
      "\n",
      "the act of scrapinga sound produced by scraping(often plural) something scraped off, together, or up; a small amount\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine = pyttsx3.init('sapi5')\n",
    "\n",
    "r1 = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print('Listening..')\n",
    "    engine.say('Listening')\n",
    "    engine.runAndWait()\n",
    "    audio = r1.listen(source)\n",
    "    audio = r1.recognize_google(audio)\n",
    "\n",
    "    if 'weather' in audio:\n",
    "        print('..')\n",
    "        words = audio.split(' ')\n",
    "        print(words[-1])\n",
    "        scrape_weather(words[-1])\n",
    "\n",
    "    elif 'covid' in audio:\n",
    "        print('..')\n",
    "        words = audio.split(' ')\n",
    "        corona_updates(words[-1])\n",
    "\n",
    "    elif 'meaning' in audio:\n",
    "        print('..')\n",
    "        words = audio.split(' ')\n",
    "        print(words[-1])\n",
    "        scrape_meaning(words[-1])\n",
    "        \n",
    "    elif 'take notes' in audio:\n",
    "        print('..')\n",
    "        take_notes()\n",
    "        print('Noted!!')\n",
    "        \n",
    "    elif 'show notes' in audio:\n",
    "        print('..')\n",
    "        show_notes()\n",
    "        print('Done')\n",
    "        \n",
    "    elif 'news' in audio:\n",
    "        print('..')\n",
    "        scrape_news()\n",
    "        \n",
    "    elif 'play' in audio:\n",
    "        print('..')\n",
    "        words = audio.split(' ')\n",
    "        print(words[-1])\n",
    "        play_youtube(audio)\n",
    "        \n",
    "    elif 'open' in audio:\n",
    "        print('..')\n",
    "        words = audio.split('open')\n",
    "        print(words[-1])\n",
    "        link = str(words[-1])\n",
    "        link = re.sub(' ', '', link)\n",
    "        engine.say('Opening')\n",
    "        engine.say(link)\n",
    "        engine.runAndWait()\n",
    "        link = f'https://{link}.com'\n",
    "        print(link)\n",
    "        webbrowser.open(link)\n",
    "        \n",
    "    elif 'where is' in audio:\n",
    "        print('..')\n",
    "        words = audio.split('where is')\n",
    "        print(words[-1])\n",
    "        link = str(words[-1])\n",
    "        link = re.sub(' ', '', link)\n",
    "        engine.say('Locating')\n",
    "        engine.say(link)\n",
    "        engine.runAndWait()\n",
    "        link = f'https://www.google.co.in/maps/place/{link}'\n",
    "        print(link)\n",
    "        webbrowser.open(link)\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        print(audio)\n",
    "        print('Sorry, I do not understand that!')\n",
    "        engine.say('Sorry, I do not understand that!')\n",
    "        engine.runAndWait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SakshiEnv] *",
   "language": "python",
   "name": "conda-env-SakshiEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
